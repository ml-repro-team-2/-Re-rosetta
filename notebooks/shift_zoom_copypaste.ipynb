{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "current = os.path.dirname(os.path.realpath(\"shift_zoom_copypaste.ipynb\"))\n",
    "parent = os.path.dirname(current)\n",
    "sys.path.append(parent)\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models import resnet50\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import clip\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch.hub\n",
    "import time\n",
    "import pickle\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from match_utils import matching, models, stats, nethook, loading, plotting, layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:4')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_biggan import (BigGAN, one_hot_from_names, truncated_noise_sample,\n",
    "                                       save_as_images, display_in_terminal)\n",
    "\n",
    "gan = BigGAN.from_pretrained('biggan-deep-256').to(device)\n",
    "\n",
    "for p in gan.parameters(): \n",
    "    p.data = p.data.float() \n",
    "    \n",
    "gan_layers = []\n",
    "for name, layer in gan.named_modules():\n",
    "    if \"conv\" in name:\n",
    "        gan_layers.append(name)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table, gan_stats, dino_stats = loading.load_stats(\"/home/amil/Rosetta/matches\", device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Buddies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_scores,_ = torch.max(table,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_matches = torch.argmax(table,1)\n",
    "dino_matches = torch.argmax(table,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfect_matches = []\n",
    "perfect_match_scores = []\n",
    "dino_perfect_matches = []\n",
    "num_perfect_matches = 0 \n",
    "for i in range(table.shape[0]):\n",
    "    gan_match = gan_matches[i].item()\n",
    "    dino_match = dino_matches[gan_match].item()\n",
    "    if dino_match == i:\n",
    "        #print(i)\n",
    "        num_perfect_matches+=1\n",
    "        perfect_matches.append(i)\n",
    "        dino_perfect_matches.append(gan_match)\n",
    "        perfect_match_scores.append(match_scores[i])\n",
    "        \n",
    "print(num_perfect_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = nethook.InstrumentedModel(gan)\n",
    "gan.retain_layers(gan_layers, detach = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, unit in enumerate(perfect_matches):\n",
    "    perfect_matches[i] = layers.find_act(perfect_matches[i],all_gan_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import truncnorm\n",
    "def truncate_noise(size, truncation):\n",
    "    '''\n",
    "    Function for creating truncated noise vectors: Given the dimensions (n_samples, z_dim)\n",
    "    and truncation value, creates a tensor of that shape filled with random\n",
    "    numbers from the truncated normal distribution.\n",
    "    Parameters:\n",
    "        n_samples: the number of samples to generate, a scalar\n",
    "        z_dim: the dimension of the noise vector, a scalar\n",
    "        truncation: the truncation value, a non-negative scalar\n",
    "    '''\n",
    "    \n",
    "    truncated_noise = truncnorm.rvs(-1*truncation, truncation, size=size)\n",
    "    \n",
    "    return torch.Tensor(truncated_noise)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = truncate_noise((1,128), 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.zeros((1,1000)).to(device)\n",
    "c[:, 207] = 1\n",
    "\n",
    "from torch.autograd import Variable\n",
    "z = Variable(z1.clone(), requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_gan_im(gan_im):\n",
    "    im = (gan_im+1)/2\n",
    "    im = torch.permute(im[0],(1,2,0)).detach().cpu()\n",
    "    plt.imshow(im)\n",
    "    plt.show()\n",
    "\n",
    "show_gan_im(gan(z,c,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_activ(input, shift_w, shift_h):\n",
    "    shifted = torch.nn.functional.pad(input[np.newaxis,:,:,:], pad=(shift_h, -shift_h, shift_w, -shift_w))\n",
    "    return shifted[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect GAN Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_activs0 = matching.store_activs(gan, gan_layers)\n",
    "gan_perfect_activs = []\n",
    "for idx in perfect_matches:\n",
    "    gan_perfect_activs.append(gan_activs0[idx[0]][:,idx[1],:,:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shift GAN Rosetta Neuron Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs = []\n",
    "for idx in perfect_matches:\n",
    "    ref = gan_activs0[idx[0]][:,idx[1],:,:].clone().double().unsqueeze(0).detach()\n",
    "    ref = shift_activ(ref, 0, int(0.25*ref.shape[2]))\n",
    "    refs.append(ref)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize for Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps=500\n",
    "lr_rampdown_length = 0.25\n",
    "lr_rampup_length = 0.05\n",
    "initial_learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([z], betas=(0.9, 0.999), lr=initial_learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_images = []\n",
    "for step in range(num_steps):\n",
    "    # Learning rate schedule.\n",
    "    t = step / num_steps\n",
    "    lr_ramp = min(1.0, (1.0 - t) / lr_rampdown_length)\n",
    "    lr_ramp = 0.5 - 0.5 * np.cos(lr_ramp * np.pi)\n",
    "    lr_ramp = lr_ramp * min(1.0, t / lr_rampup_length)\n",
    "    lr = initial_learning_rate * lr_ramp\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    # Synth images from opt_w.\n",
    "    synth_images = gan(z,c,1)\n",
    "\n",
    "\n",
    "    # track images\n",
    "    synth_images = (synth_images + 1) * (255/2)\n",
    "    synth_images_np = synth_images.clone().detach().permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n",
    "    all_images.append(synth_images_np)\n",
    "\n",
    "    # Downsample image to 256x256 if it's larger than that. VGG was built for 224x224 images.\n",
    "    if synth_images.shape[2] > 256:\n",
    "        synth_images = F.interpolate(synth_images, size=(256, 256), mode='area')\n",
    "\n",
    "\n",
    "    gan_activs1 = matching.store_activs(gan, gan_layers)\n",
    "    #normalize all activations\n",
    "    eps = 0.00001\n",
    "    for i,_ in enumerate(gan_activs1):\n",
    "        gan_activs1[i] = (gan_activs1[i]-gan_stats[i][0])/(gan_stats[i][1]+eps)\n",
    "\n",
    "    gan_perfect_activs1 = []\n",
    "    for idx in perfect_matches:\n",
    "        gan_perfect_activs1.append(gan_activs1[idx[0]][:,idx[1],:,:])\n",
    "\n",
    "\n",
    "\n",
    "    #pearson correlation\n",
    "    a_loss = 0\n",
    "    for i in range(len(perfect_matches)):\n",
    "        map_size = gan_perfect_activs1[i].shape[1] #max((gan_perfect_activs[i].shape[1], ref.shape[1]))\n",
    "        gan_activ_new = torch.nn.Upsample(size=(map_size,map_size), mode='bilinear')(gan_perfect_activs1[i].unsqueeze(0)).double()\n",
    "        prod = torch.einsum('aixy,ajxy->ij', gan_activ_new, refs[i])\n",
    "        div1 = torch.sum(gan_activ_new**2)\n",
    "        div2 = torch.sum(ref**2)\n",
    "        corr = prod/torch.sqrt(div1*div2)\n",
    "        a_loss += corr\n",
    "\n",
    "\n",
    "    a_loss *= -1 \n",
    "    l_reg = torch.mean((z - z1)**2)\n",
    "    # Features for synth images.\n",
    "    coeff = 0.5 #10\n",
    "    loss = a_loss #+ coeff * l_reg\n",
    "    # Step\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    msg  = f'[ step {step+1:>4d}/{num_steps}] '\n",
    "    msg += f'[ a_loss: {float(a_loss):5.2f} loss_reg: {coeff * float(l_reg):5.2f}] '\n",
    "    print(msg)\n",
    "    if step % 10 == 0:\n",
    "        plt.imshow(synth_images_np)\n",
    "        plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shift Other Way and Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs2 = []\n",
    "for idx in perfect_matches:\n",
    "    ref = gan_activs0[idx[0]][:,idx[1],:,:].clone().double().unsqueeze(0).detach()\n",
    "    ref = shift_activ(ref, 0, -int(0.25*ref.shape[2]))\n",
    "    refs2.append(ref)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = Variable(z1.clone(), requires_grad=True)\n",
    "optimizer = torch.optim.Adam([z], betas=(0.9, 0.999), lr=initial_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = []\n",
    "for step in range(num_steps):\n",
    "    # Learning rate schedule.\n",
    "    t = step / num_steps\n",
    "    lr_ramp = min(1.0, (1.0 - t) / lr_rampdown_length)\n",
    "    lr_ramp = 0.5 - 0.5 * np.cos(lr_ramp * np.pi)\n",
    "    lr_ramp = lr_ramp * min(1.0, t / lr_rampup_length)\n",
    "    lr = initial_learning_rate * lr_ramp\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    # Synth images from opt_w.\n",
    "    synth_images = gan(z,c,1)\n",
    "\n",
    "\n",
    "    # track images\n",
    "    synth_images = (synth_images + 1) * (255/2)\n",
    "    synth_images_np = synth_images.clone().detach().permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n",
    "    all_images.append(synth_images_np)\n",
    "\n",
    "    # Downsample image to 256x256 if it's larger than that. VGG was built for 224x224 images.\n",
    "    if synth_images.shape[2] > 256:\n",
    "        synth_images = F.interpolate(synth_images, size=(256, 256), mode='area')\n",
    "\n",
    "\n",
    "    gan_activs2 = matching.store_activs(gan, gan_layers)\n",
    "    #normalize all activations\n",
    "    eps = 0.00001\n",
    "    for i,_ in enumerate(gan_activs2):\n",
    "        gan_activs2[i] = (gan_activs2[i]-gan_stats[i][0])/(gan_stats[i][1]+eps)\n",
    "\n",
    "    gan_perfect_activs2 = []\n",
    "    for idx in perfect_matches:\n",
    "        gan_perfect_activs2.append(gan_activs2[idx[0]][:,idx[1],:,:])\n",
    "\n",
    "\n",
    "\n",
    "    #pearson correlation\n",
    "    a_loss = 0\n",
    "    for i in range(len(perfect_matches)):\n",
    "        map_size = gan_perfect_activs2[i].shape[1] #max((gan_perfect_activs[i].shape[1], ref.shape[1]))\n",
    "        gan_activ_new = torch.nn.Upsample(size=(map_size,map_size), mode='bilinear')(gan_perfect_activs2[i].unsqueeze(0)).double()\n",
    "        prod = torch.einsum('aixy,ajxy->ij', gan_activ_new, refs2[i])\n",
    "        div1 = torch.sum(gan_activ_new**2)\n",
    "        div2 = torch.sum(ref**2)\n",
    "        corr = prod/torch.sqrt(div1*div2)\n",
    "        a_loss += corr\n",
    "\n",
    "\n",
    "    a_loss *= -1 \n",
    "    l_reg = torch.mean((z - z1)**2)\n",
    "    # Features for synth images.\n",
    "    coeff = 0.5 #10\n",
    "    loss = a_loss #+ coeff * l_reg\n",
    "    # Step\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    msg  = f'[ step {step+1:>4d}/{num_steps}] '\n",
    "    msg += f'[ a_loss: {float(a_loss):5.2f} loss_reg: {coeff * float(l_reg):5.2f}] '\n",
    "    print(msg)\n",
    "    if step % 10 == 0:\n",
    "        plt.imshow(synth_images_np)\n",
    "        plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Activations to Copy and Paste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs3 = []\n",
    "for r, l in zip(gan_perfect_activs1, gan_perfect_activs2):\n",
    "    x = torch.zeros_like(l)#*torch.min(l).detach()\n",
    "    x[:,:,:2*int(x.shape[-1])//4] = l[:,:,:2*int(x.shape[-1])//4].detach().clone()\n",
    "    x[:,:,2*int(x.shape[-1])//4:] = r[:,:,2*int(x.shape[-1])//4:].detach().clone()\n",
    "    refs3.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = Variable(truncate_noise((1,128), 1).to(device), requires_grad=True)\n",
    "initial_learning_rate = 0.01\n",
    "optimizer = torch.optim.Adam([z], betas=(0.9, 0.999), lr=initial_learning_rate)\n",
    "c = torch.zeros((1,1000)).to(device)\n",
    "c[:, 207] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = []\n",
    "num_steps =1000\n",
    "for step in range(num_steps):\n",
    "    # Learning rate schedule.\n",
    "    t = step / num_steps\n",
    "    lr_ramp = min(1.0, (1.0 - t) / lr_rampdown_length)\n",
    "    lr_ramp = 0.5 - 0.5 * np.cos(lr_ramp * np.pi)\n",
    "    lr_ramp = lr_ramp * min(1.0, t / lr_rampup_length)\n",
    "    lr = initial_learning_rate * lr_ramp\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    # Synth images from opt_w.\n",
    "    synth_images = gan(z,c,1)\n",
    "\n",
    "\n",
    "    # track images\n",
    "    synth_images = (synth_images + 1) * (255/2)\n",
    "    synth_images_np = synth_images.clone().detach().permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n",
    "    all_images.append(synth_images_np)\n",
    "\n",
    "    # Downsample image to 256x256 if it's larger than that. VGG was built for 224x224 images.\n",
    "    if synth_images.shape[2] > 256:\n",
    "        synth_images = F.interpolate(synth_images, size=(256, 256), mode='area')\n",
    "\n",
    "\n",
    "    gan_activs3 = matching.store_activs(gan, gan_layers)\n",
    "    #normalize all activations\n",
    "    eps = 0.00001\n",
    "    for i,_ in enumerate(gan_activs3):\n",
    "        gan_activs3[i] = (gan_activs3[i]-gan_stats[i][0])/(gan_stats[i][1]+eps)\n",
    "\n",
    "    gan_perfect_activs3 = []\n",
    "    for idx in perfect_matches:\n",
    "        gan_perfect_activs3.append(gan_activs3[idx[0]][:,idx[1],:,:])\n",
    "\n",
    "\n",
    "\n",
    "    #pearson correlation\n",
    "    a_loss = 0\n",
    "    for i in range(len(perfect_matches)):\n",
    "        map_size = gan_perfect_activs3[i].shape[1] #max((gan_perfect_activs[i].shape[1], ref.shape[1]))\n",
    "        gan_activ_new = torch.nn.Upsample(size=(map_size,map_size), mode='bilinear')(gan_perfect_activs3[i].unsqueeze(0)).double()\n",
    "        prod = torch.einsum('aixy,ajxy->ij', gan_activ_new, refs3[i].unsqueeze(0))\n",
    "        div1 = torch.sum(gan_activ_new**2)\n",
    "        div2 = torch.sum(ref**2)\n",
    "        corr = prod/torch.sqrt(div1*div2)\n",
    "        a_loss += corr\n",
    "\n",
    "\n",
    "    a_loss *= -1 \n",
    "    l_reg = torch.mean((z - z1)**2)\n",
    "    # Features for synth images.\n",
    "    coeff = 0.5 #10\n",
    "    loss = a_loss# + coeff * l_reg\n",
    "    # Step\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    msg  = f'[ step {step+1:>4d}/{num_steps}] '\n",
    "    msg += f'[ a_loss: {float(a_loss):5.2f} loss_reg: {coeff * float(l_reg):5.2f}] '\n",
    "    print(msg)\n",
    "    if step % 10 == 0:\n",
    "        plt.imshow(synth_images_np)\n",
    "        plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoom(input, scale):\n",
    "    activ_res = input.shape[-1]\n",
    "    zoomed = F.interpolate(input[np.newaxis,:,:,:], scale_factor=scale)\n",
    "    tmp_res = zoomed.shape[-1]\n",
    "    pad = (tmp_res - activ_res) // 2\n",
    "    zoomed = zoomed[:, :, pad:pad+activ_res, pad:pad+activ_res]\n",
    "    return zoomed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs4 = []\n",
    "for idx in perfect_matches:\n",
    "    ref = gan_activs0[idx[0]][:,idx[1],:,:].clone().double().detach()\n",
    "    ref = zoom(ref,2)\n",
    "    refs4.append(ref)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.001\n",
    "z = Variable(z1.clone(), requires_grad=True)\n",
    "optimizer = torch.optim.Adam([z], betas=(0.9, 0.999), lr=initial_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = []\n",
    "num_steps =500\n",
    "for step in range(num_steps):\n",
    "    # Learning rate schedule.\n",
    "    t = step / num_steps\n",
    "    lr_ramp = min(1.0, (1.0 - t) / lr_rampdown_length)\n",
    "    lr_ramp = 0.5 - 0.5 * np.cos(lr_ramp * np.pi)\n",
    "    lr_ramp = lr_ramp * min(1.0, t / lr_rampup_length)\n",
    "    lr = initial_learning_rate * lr_ramp\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    # Synth images from opt_w.\n",
    "    synth_images = gan(z,c,1)\n",
    "\n",
    "\n",
    "    # track images\n",
    "    synth_images = (synth_images + 1) * (255/2)\n",
    "    synth_images_np = synth_images.clone().detach().permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n",
    "    all_images.append(synth_images_np)\n",
    "\n",
    "    # Downsample image to 256x256 if it's larger than that. VGG was built for 224x224 images.\n",
    "    if synth_images.shape[2] > 256:\n",
    "        synth_images = F.interpolate(synth_images, size=(256, 256), mode='area')\n",
    "\n",
    "\n",
    "    gan_activs4 = matching.store_activs(gan, gan_layers)\n",
    "    #normalize all activations\n",
    "    eps = 0.00001\n",
    "    for i,_ in enumerate(gan_activs4):\n",
    "        gan_activs4[i] = (gan_activs4[i]-gan_stats[i][0])/(gan_stats[i][1]+eps)\n",
    "\n",
    "    gan_perfect_activs4 = []\n",
    "    for idx in perfect_matches:\n",
    "        gan_perfect_activs4.append(gan_activs4[idx[0]][:,idx[1],:,:])\n",
    "\n",
    "\n",
    "\n",
    "    #pearson correlation\n",
    "    a_loss = 0\n",
    "    for i in range(len(perfect_matches)):\n",
    "        map_size = gan_perfect_activs4[i].shape[1] #max((gan_perfect_activs[i].shape[1], ref.shape[1]))\n",
    "        gan_activ_new = torch.nn.Upsample(size=(map_size,map_size), mode='bilinear')(gan_perfect_activs4[i].unsqueeze(0)).double()\n",
    "        prod = torch.einsum('aixy,ajxy->ij', gan_activ_new, refs4[i].unsqueeze(0))\n",
    "        div1 = torch.sum(gan_activ_new**2)\n",
    "        div2 = torch.sum(ref**2)\n",
    "        corr = prod/torch.sqrt(div1*div2)\n",
    "        a_loss += corr\n",
    "\n",
    "\n",
    "    a_loss *= -1 \n",
    "    l_reg = torch.mean((z - z1)**2)\n",
    "    # Features for synth images.\n",
    "    coeff = 10 #10\n",
    "    loss = a_loss + coeff * l_reg\n",
    "    # Step\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    msg  = f'[ step {step+1:>4d}/{num_steps}] '\n",
    "    msg += f'[ a_loss: {float(a_loss):5.2f} loss_reg: {coeff * float(l_reg):5.2f}] '\n",
    "    print(msg)\n",
    "    if step % 10 == 0:\n",
    "        plt.imshow(synth_images_np)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
